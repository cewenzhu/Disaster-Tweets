{"cells":[{"cell_type":"code","execution_count":null,"id":"983b04c9","metadata":{"id":"983b04c9","outputId":"c989d083-320a-413f-fc0e-cb02ff2ab36b"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/chuqinwu/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     /Users/chuqinwu/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     /Users/chuqinwu/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /Users/chuqinwu/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]}],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import re\n","import time\n","import emoji\n","import string\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","from nltk.corpus import stopwords\n","from nltk.tokenize import WhitespaceTokenizer\n","from nltk.corpus import wordnet\n","from nltk.stem import WordNetLemmatizer"]},{"cell_type":"code","execution_count":null,"id":"84ffad24","metadata":{"id":"84ffad24","outputId":"91318947-b174-4886-cdaf-e7209ce5b25b"},"outputs":[{"name":"stdout","output_type":"stream","text":["data/test.csv\n","data/train.csv\n","data/sample_submission.csv\n"]}],"source":["for dirname, _, filenames in os.walk('data/'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"]},{"cell_type":"code","execution_count":null,"id":"4efffd26","metadata":{"id":"4efffd26"},"outputs":[],"source":["train_df = pd.read_csv('data/train.csv')\n","test_df = pd.read_csv('data/test.csv')"]},{"cell_type":"code","execution_count":null,"id":"f3769371","metadata":{"id":"f3769371","outputId":"b4541914-e4d2-4380-cd40-e8b7a0153447"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Our Deeds are the Reason of this #earthquake M...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>All residents asked to 'shelter in place' are ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7608</th>\n","      <td>10869</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Two giant cranes holding a bridge collapse int...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7609</th>\n","      <td>10870</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7610</th>\n","      <td>10871</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7611</th>\n","      <td>10872</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Police investigating after an e-bike collided ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7612</th>\n","      <td>10873</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>The Latest: More Homes Razed by Northern Calif...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7613 rows Ã— 5 columns</p>\n","</div>"],"text/plain":["         id keyword location  \\\n","0         1     NaN      NaN   \n","1         4     NaN      NaN   \n","2         5     NaN      NaN   \n","3         6     NaN      NaN   \n","4         7     NaN      NaN   \n","...     ...     ...      ...   \n","7608  10869     NaN      NaN   \n","7609  10870     NaN      NaN   \n","7610  10871     NaN      NaN   \n","7611  10872     NaN      NaN   \n","7612  10873     NaN      NaN   \n","\n","                                                   text  target  \n","0     Our Deeds are the Reason of this #earthquake M...       1  \n","1                Forest fire near La Ronge Sask. Canada       1  \n","2     All residents asked to 'shelter in place' are ...       1  \n","3     13,000 people receive #wildfires evacuation or...       1  \n","4     Just got sent this photo from Ruby #Alaska as ...       1  \n","...                                                 ...     ...  \n","7608  Two giant cranes holding a bridge collapse int...       1  \n","7609  @aria_ahrary @TheTawniest The out of control w...       1  \n","7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n","7611  Police investigating after an e-bike collided ...       1  \n","7612  The Latest: More Homes Razed by Northern Calif...       1  \n","\n","[7613 rows x 5 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["train_df"]},{"cell_type":"code","execution_count":null,"id":"ebe652e6","metadata":{"id":"ebe652e6","outputId":"488f64de-ec6c-47b0-bd8a-25396cb64c25"},"outputs":[{"data":{"text/plain":["7503"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["train_df = train_df.drop_duplicates('text',ignore_index=True)\n","len(train_df)"]},{"cell_type":"code","execution_count":null,"id":"f1dd1eac","metadata":{"id":"f1dd1eac"},"outputs":[],"source":["\n","def removeEmoji(text):\n","    return emoji.replace_emoji(text, '')\n","\n","#https://stackoverflow.com/questions/15586721/wordnet-lemmatization-and-pos-tagging-in-python\n","def get_wordnet_pos(treebank_tag):\n","\n","    if treebank_tag.startswith('J'):\n","        return wordnet.ADJ\n","    elif treebank_tag.startswith('V'):\n","        return wordnet.VERB\n","    elif treebank_tag.startswith('N'):\n","        return wordnet.NOUN\n","    elif treebank_tag.startswith('R'):\n","        return wordnet.ADV\n","    else:\n","        return wordnet.NOUN\n","\n","def tweet_cleaner(text):\n","    text = text.lower() #convert to lowercase\n","    text = re.sub(\" \\d+\", \" \", text)\n","    text = text.translate(str.maketrans(\"\",\"\", string.punctuation)) #remove punctuation\n","    text = removeEmoji(text) #remove emoji\n","    tk = WhitespaceTokenizer()  #tokenize text to list of words without space\n","    textsplit = tk.tokenize(text)\n","    lemmatizer = WordNetLemmatizer()\n","    tag = nltk.pos_tag(textsplit) #get tags (noun, verb...)\n","\n","    result = []\n","    for i,w in enumerate(tag):\n","        word = w[0]\n","        tag = w[1]\n","        if word not in set(stopwords.words(\"english\")) and not word.startswith(\"http\") and \"\\\\\" not in word and '#' not in word and '@' not in word:\n","            result.append(lemmatizer.lemmatize(word,get_wordnet_pos(tag))) #lemmartize word to current tense\n","\n","        #text = ' '.join(result) #join back the result\n","    return result"]},{"cell_type":"code","execution_count":null,"id":"2ea72e97","metadata":{"id":"2ea72e97"},"outputs":[],"source":["from collections import Counter\n","def bag_of_words(ls):\n","    return Counter(ls)\n","\n","# Taken from http://web.stanford.edu/class/cs221/ Assignment #2 Support Code\n","def dotProduct(d1, d2):\n","    \"\"\"\n","    @param dict d1: a feature vector represented by a mapping from a feature (string) to a weight (float).\n","    @param dict d2: same as d1\n","    @return float: the dot product between d1 and d2\n","    \"\"\"\n","    if len(d1) < len(d2):\n","        return dotProduct(d2, d1)\n","    else:\n","        return sum(d1.get(f, 0) * v for f, v in d2.items())\n","\n","def increment(d1, scale, d2):\n","    \"\"\"\n","    Implements d1 += scale * d2 for sparse vectors.\n","    @param dict d1: the feature vector which is mutated.\n","    @param float scale\n","    @param dict d2: a feature vector.\n","\n","    NOTE: This function does not return anything, but rather\n","    increments d1 in place. We do this because it is much faster to\n","    change elements of d1 in place than to build a new dictionary and\n","    return it.\n","    \"\"\"\n","    for f, v in d2.items():\n","        d1[f] = d1.get(f, 0) + v * scale\n","\n","def pegasos_sw(lbd,X,y,epoch):\n","    \n","    w = {}\n","    s = 1\n","    t = 0\n","    for e in range(epoch):\n","        temp = {}\n","        for j in range(len(y)):\n","            t += 1\n","            elta = 1/(t*lbd)\n","            d = X[j]\n","           \n","            for k in d.keys():\n","                if k not in w:\n","                    w[k] = 0\n","\n","            result = s*y[j]*dotProduct(w, d)\n","            s = s * (1 - elta * lbd)\n","            \n","            if s == 0: s = 1\n","            if result < 1:\n","                increment(w, (1/s)*elta*y[j], d)\n","        \n","            #print({k: s*w[k] for k in list(w.keys())[:3]})\n","            \n","    for f, v in w.items():\n","        w[f] = v * s\n","            \n","    return w\n","\n","def classification_error(w,X,y):\n","    loss = 0\n","    for j in range(len(y)):\n","        d = bag_of_words(X[j])\n","        result = dotProduct(w, d)\n","       \n","        if result > 0 and y[j] == -1:\n","            loss += 1\n","        elif result < 0 and y[j] == 1:\n","            loss += 1\n","    return loss/len(y)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"cba49b3e","metadata":{"id":"cba49b3e"},"outputs":[],"source":["def train_test_split(data,target):\n","    X_train = []\n","    y_train = []\n","    for i in range(len(data)):\n","        X_train.append(bag_of_words(data[i]))\n","        if target[i] == 0:\n","            y_train.append(-1)\n","        else:\n","            y_train.append(1)\n","        \n","    return X_train, y_train"]},{"cell_type":"code","execution_count":null,"id":"a9c148a5","metadata":{"id":"a9c148a5"},"outputs":[],"source":["result = []\n","for index, tweet in enumerate(train_df['text']):\n","    tweet = tweet_cleaner(tweet)\n","    result.append(tweet)\n"]},{"cell_type":"code","execution_count":null,"id":"9dc26eb7","metadata":{"id":"9dc26eb7"},"outputs":[],"source":["X_train, y_train = train_test_split(result,train_df['target'])"]},{"cell_type":"code","execution_count":null,"id":"196d7aa8","metadata":{"id":"196d7aa8","outputId":"a61c6a90-9cf5-4de2-a6a9-55dbbc21c62f"},"outputs":[{"data":{"text/plain":["0.1643342662934826"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["w = pegasos_sw(0.01,X_train, y_train,10)\n","classification_error(w,X_train,y_train)"]},{"cell_type":"code","execution_count":null,"id":"a07d1604","metadata":{"id":"a07d1604"},"outputs":[],"source":["test_result = []\n","for index, tweet in enumerate(test_df['text']):\n","    tweet = tweet_cleaner(tweet)\n","    test_result.append(tweet)"]},{"cell_type":"code","execution_count":null,"id":"40d0f1ed","metadata":{"id":"40d0f1ed"},"outputs":[],"source":["X_test = []\n","for i in range(len(test_result)):\n","    X_test.append(bag_of_words(test_result[i]))"]},{"cell_type":"code","execution_count":null,"id":"f9a388d3","metadata":{"id":"f9a388d3"},"outputs":[],"source":["prediction = []\n","for j in range(len(X_test)):\n","    d = X_test[j]\n","    result = dotProduct(w, d)\n","    if result >= 0:\n","        prediction.append(1)\n","    else:\n","        prediction.append(0)"]},{"cell_type":"code","execution_count":null,"id":"d80f4360","metadata":{"id":"d80f4360"},"outputs":[],"source":["prediction = pd.DataFrame(prediction)\n","prediction.columns = ['target']"]},{"cell_type":"code","execution_count":null,"id":"a5bdf391","metadata":{"id":"a5bdf391"},"outputs":[],"source":["prediction['id'] = test_df['id']"]},{"cell_type":"code","execution_count":null,"id":"83510717","metadata":{"id":"83510717","outputId":"96a9a6b6-a9e0-48bb-ab52-4c14f97382bc"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>target</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>10861</th>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>10865</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10868</th>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>10874</th>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>10875</th>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3263 rows Ã— 1 columns</p>\n","</div>"],"text/plain":["       target\n","id           \n","0           1\n","2           1\n","3           1\n","9           1\n","11          1\n","...       ...\n","10861       1\n","10865       0\n","10868       1\n","10874       1\n","10875       1\n","\n","[3263 rows x 1 columns]"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["prediction.set_index('id',inplace=True)\n","prediction"]},{"cell_type":"code","execution_count":null,"id":"7a299ab4","metadata":{"id":"7a299ab4"},"outputs":[],"source":["prediction.to_csv('prediction.csv')"]},{"cell_type":"code","execution_count":null,"id":"a2d94b72","metadata":{"id":"a2d94b72"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}